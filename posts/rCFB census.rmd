---
title: "r/CFB Census"
output:
  md_document:
    variant: markdown_github
date: "2023-09-26"
---

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
library(dplyr)
library(tidyr)
library(tibble)
library(lubridate)
library(ggplot2)
library(stringr)
library(here)
library(fs)

devtools::load_all()

# Load data --------------------------------------------------------------------

list_data <- list.files(here("data", "reddit-comment-data", "cfb", "2023"),
                        pattern = "*.rds", full.names = T, recursive = T)
  # str_replace("-2023.*", "") |>
  # str_replace(".*/", "")

all_data <- lapply(list_data, readr::read_rds) |>
  dplyr::bind_rows()

gt_data <- all_data |>
  filter(str_detect(title, "@|vs|v.s") & str_detect(title, "\\[Game Thread"))

pgt_data <- all_data |>
  filter(str_detect(title, "\\[Postgame Thread"))

clean_rcfb_comments <- function(data) {

  swear_strings <- paste("(?i)\\b(fuc(k|c|))\\b",
                    "\\b(wtf)\\b",
                    "\\b(ass(hole|es|))\\b",
                    "damn",
                    "shit",
                    "\\b(hell)\\b",
                    "bitch",
                    "bastard",
                    sep = "|")
  ref_strings <- paste("(?i)\\b(ref(s|erees|effing|efball|))\\b", # ref, refs, referees, reffing, refball, etc.
                       "\\b(officials)\\b", # officials -- singluar has too many false positives
                       "\\b(flag(s|))\\b", # flag(s)
                       # "\\b(calls)\\b", # calls -- others have too many false positives
                       "\\b(whistle(s|))\\b", # whistle(s)
                       "the fix", # the fix, fixed
                       "\\b(rigging|rig|rigged)\\b", # rig, rigged, rigging
                       # "\\b(spot|spotted|spots)\\b", # spot, spots, spotted
                       "\\b(hosed|jobbed|robbed)\\b", # hosed, jobbed, robbed
                       sep = "|")
  ad_strings <- paste("(?i)\\b(ad|ads|advertisements)\\b",
                      "\\b(commercial|comer.*al|commer.*al)\\b",
                      "\\b(progressive|all state|cheez|att|at&t|lilly|fansville|fansvile|dr\\. pepper|dr pepper|burger king)\\b",
                      sep = "|")

data_clean <- data |>
    mutate(
      # Cleaning
      time = as_datetime(time_unix),
      time_cst = lubridate::as_datetime(time_unix) |> with_tz("America/Chicago"),
      flair = trimws(str_remove(flair, "\\:[^()]*\\:")),
      flair = if_else(flair == "NULL", "No Flair", flair), # No flairs
      flair = if_else(flair == "Go to https://flair.redditcfb.com to get your flair!", "/r/CFB", flair),
      title_clean = trimws(str_remove_all(title, "\\([^)]*\\)|\\[[^]]*\\]|\\d+|[^A-Za-z@\\s]")),
      # Ref complaints
      ref_complaint = if_else(str_detect(body, ref_strings), TRUE, FALSE),
      # Swears
      swear = if_else(str_detect(body, swear_strings), TRUE, FALSE),
      # Commercials
      ad_complaint = if_else(str_detect(body, ad_strings), TRUE, FALSE)
    ) |>
    # separate(col = flair, sep = " â€¢ ", into = c("flair_one", "flair_two")) |>
    separate(col = title_clean, sep = " (@|Defeats) ", into = c("away", "home"), remove = FALSE) |>
    separate_wider_delim(cols = flair,
                         delim = " â€¢ ",
                         names = c("flair_one", "flair_two"),
                         too_few = "align_start",
                         cols_remove = F
    ) |>
    # separate_wider_delim(cols = title_clean,
    #                      delim = "@|defeats",
    #                      names = c("away", "home"),
    #                      too_few = "align_start",
    #                      cols_remove = F
    # ) |>
    mutate(
      flair_one = if_else(is.na(flair_one) | flair_one == "", "Unflaired", flair_one),
      away = str_remove(away, "\\[Game Thread]"),
      away = trimws(str_replace_all(away, pattern = "[^a-zA-Z ]", "")),
      home = str_remove(home, " \\s*\\([^\\)]+\\)"),
      home = trimws(str_replace_all(home, pattern = "[^a-zA-Z ]", ""))
    ) # |> suppressWarnings()

}

all_data_clean <- all_data |>
  clean_rcfb_comments()

gt_data_clean <- all_data_clean |>
  filter(str_detect(title, "@|vs|v.s") & str_detect(title, "\\[Game Thread"))

pgt_data_clean <- all_data_clean |>
  filter(str_detect(title, "\\[Postgame Thread"))

# Total threads:
n_gts <- gt_data_clean |> count(title, sort = T) |> nrow() |> format(big.mark = ",")
n_pgts <- pgt_data_clean |> count(title, sort = T) |> nrow() |> format(big.mark = ",")
n_all <- all_data_clean |> count(title, sort = T) |> nrow() |> format(big.mark = ",")

```

# What is this?

As a side project, I decided to scrape every comment from every game thread this season. I've done this here and there in the past, but I've never done it over a whole season. I've got my setup fully automated to scrape every thread with "[Game Thread]" in the title at 4AM every night, and it's been a success over the first few weeks.

So, I thought it would be fun to share what I've got so far. I used the data to come up with two things that I thought were kinda fun:

-   A "r/CFB flair census", which counts the number of unique users with each primary flair (i.e. how big is each fanbase on the sub). I've always wondered about this.
    -   I also used the data to see which fanbase swears the least/most, who comments the most often, and a few other honorifics that you can either take pride in or mock your rivals for.
    -   You can also see each flair's #1 most talkative fan! If you work hard, you could see your name up there at the end of the season.
-   A "r/CFB posting leaderboard", which tracks which individual poster has commented in game threads the most throughout the whole season. Sadly I did not make the top 25.
    -   I also crowned the #1 sicko, as measured by unique number of threads participated in. Something else to aspire to!

# The data

So far, I've scraped `r n_gts` game threads with `r nrow(gt_data_clean) |> format(big.mark = ",")` total comments. I actually also have the post game threads as well, which expands the total number of comments to `r nrow(all_data_clean) |> format(big.mark = ",")`, but for now I'm just going to focus on the game threads. In my mind it makes sense to treat them as separate things, but if anyone wants to see any of this with the combined / PGT-only data, just let me know.

Here's a fun graph that shows how many comments people left per hour throughout the whole season: [GRAPH: comments per hour](https://imgur.com/8jnETSl)

Here's a table with the top ten game threads so far this year by total comments. Colorado seems to be popular for some reason, not sure why.

```{r table6, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
gt_data_clean |>
  count(title_clean, sort = T) |>
  slice_max(order_by = n,
            n = 10) |>
  mutate(n = format(n, big.mark = ",")) |>
  rename("Thread" = title_clean,
         "Total Comments" = n) |>
  knitr::kable(format = "pipe")

```

```{r calc, echo=FALSE, warning=FALSE, message=FALSE}
# Census =======================================================================

unique_users <- gt_data_clean |>
  group_by(author) |>
  summarize(
    n_comments = n(),
    n_comments_formatted = format(n_comments, big.mark = ","),
    avg_score = mean(score, na.rm = T),
    n_downvoted = sum(score < 1, na.rm = T),
    p_downvoted = n_downvoted / n_comments,
    n_unique_threads = n_distinct(title_clean, na.rm = T),
    all_primary_flairs = paste0(unique(flair_one), collapse = ", "),
    all_secondary_flairs = paste0(unique(flair_two), collapse = ", "),
    flair_list = list(unique(flair_one)),
    # TODO: replace this with something where instead of first() it's explicitly
    # looping through and pulling out the first one that's not "Unflaired"
    counted_flair = purrr::map_chr(flair_list, first),
    flaired_up = if_else(str_detect(all_primary_flairs, ","), TRUE, FALSE),
    n_swears = sum(swear, na.rm = T),
    p_swears = sum(n_swears) / n_comments,
    n_ref_comps = sum(ref_complaint, na.rm = T),
    p_ref_comps = sum(n_ref_comps) / n_comments,
    n_ads = sum(ad_complaint, na.rm = T),
    p_ads = sum(n_ads) / n_comments
  )

# Top users
summary_users <- unique_users |>
  # slice_max(order_by = n_comments, n = 25)
  arrange(desc(n_comments))

summary_users_formatted  <- summary_users |>
  mutate(
    rank = paste0("#", row_number()),
    p_swears = paste0(100 * round(p_swears, 4), "%"),
    p_ref_comps = paste0(100 * round(p_ref_comps, 4), "%"),
    # p_ads = paste0(100 * round(p_ads, 4), "%")
  ) |>
  slice_max(order_by = n_comments, n = 25) |>
  select(
    "Rank" = rank,
    "Poster" = author,
    "Primary Flair" = counted_flair,
    "Total Comments" = n_comments_formatted,
    # "Avg. Score" = avg_score,
    "Unique Threads" = n_unique_threads,
    "% Comments w/ Swears" = p_swears,
    "% Comments w/ Ref Complaints" = p_ref_comps,
    # "% Comments w/ Ad Complaints" = p_ads
  )

# Flair breakdown --------------------------------------------------------------
summary_flair <- unique_users |>
  group_by(counted_flair) |>
  summarize(
    n_unique_users = n(),
    n_total_comments = sum(n_comments, na.rm = T),
    avg_comments_per_user = n_total_comments / n_unique_users,
    avg_avg_score = mean(avg_score, na.rm = T),
    p_swears = sum(n_swears) / n_total_comments,
    p_ref_comps = sum(n_ref_comps) / n_total_comments,
    # p_ads = sum(n_ads) / n_total_comments
  )
  # arrange(desc(n_unique_users)) |>
  # slice_max(n = 100, order_by = n_unique_users) |>
  # Formatting

top_users <- unique_users |>
  group_by(counted_flair) |>
  filter(
    n_comments == max(n_comments)
  ) |>
  mutate(author = paste0(author, " (", n_comments_formatted, " comments)")) |>
  select(counted_flair, "top_poster" = author)

summary_flair_formatted <- summary_flair |>
  filter(n_unique_users >= 50) |>
  arrange(desc(n_unique_users)) |>
  mutate(
    across(.cols = !c(counted_flair, p_swears, p_ref_comps),
           .fns = ~format(round(.x, 2), big.mark = ",")),
    p_swears = paste0(100 * round(p_swears, 4), "%"),
    p_ref_comps = paste0(100 * round(p_ref_comps, 4), "%")
  ) |>
  # Add top user
  left_join(top_users, by = "counted_flair") |>
  filter(counted_flair != "Unflaired") |>
  mutate(
    # counted_flair = if_else(counted_flair == "Unflaired", "ðŸ¤® Unflaired ðŸ¤®", counted_flair),
    rank = paste0("#", row_number())
    ) |>
  # slice_max(n = 100, order_by = n_unique_users) |>
  select(
    "Rank" = rank,
    "Primary Flair" = counted_flair,
    "Unique Users" = n_unique_users,
    "Total Comments" = n_total_comments,
    "Comments per User" = avg_comments_per_user,
    "Avg. Comment Score" = avg_avg_score,
    "% of Comments w/ Swears" = p_swears,
    "% of Comments w/ Ref Complaints" = p_ref_comps,
    "Top Poster" = top_poster
  )

# How many flaired up in the dataset?
n_flaired_up <- sum(unique_users$flaired_up)

summary_flaired_up <- unique_users |>
  filter(flaired_up) |>
  count(counted_flair, sort = T) |>
  left_join(summary_flair |>
              select(counted_flair, n_unique_users)) |>
  mutate(new_flairs_percap = n / n_unique_users)

```

# r/CFB flair census

The first question I wanted to answer was "how big is each fanbase?" To determine this, I took every username that left a comment in a game thread this year, and classified them by their most recent primary flair. Also, just to save space, I cut it off at flairs with 50+ unique users. Feel free to ask if you're curious about a smaller school's numbers.

The first table here shows the results, sorted by total unique users:

## Flair census

```{r table1, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
summary_flair_formatted |>
  knitr::kable(format = "pipe")
```

Alabama is only putting up a paltry 3.58 upvotes per post. Is the Saban era finally over?

## Top 10 most / least chatty flair

```{r table1_5, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
summary_flair |>
  filter(n_unique_users >= 50) |>
  slice_max(order_by = avg_comments_per_user,
            n = 10) |>
  mutate(rank = paste0("#", row_number())) |>
  transmute("Rank" = rank,
            "Flair" = counted_flair,
            "Total Comments" = format(n_total_comments, big.mark = ","),
            "Comments per User" = format(round(avg_comments_per_user, 1), big.mark = ",")
  ) |>
  knitr::kable(format = "pipe")

summary_flair |>
  filter(n_unique_users >= 50) |>
  slice_max(order_by = -avg_comments_per_user,
            n = 10) |>
  mutate(rank = paste0("#", row_number())) |>
  transmute("Rank" = rank,
            "Flair" = counted_flair,
            "Total Comments" = format(n_total_comments, big.mark = ","),
            "Comments per User" = format(round(avg_comments_per_user, 1), big.mark = ",")
  ) |>
  knitr::kable(format = "pipe")
```

## Top 10 most / least swears

```{r table2, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
summary_flair |>
  filter(n_unique_users >= 50) |>
  slice_max(order_by = p_swears,
            n = 10) |>
  mutate(rank = paste0("#", row_number())) |>
  transmute("Rank" = rank,
            "Flair" = counted_flair,
            "Total Comments" = format(n_total_comments, big.mark = ","),
            "% Comments w/ Swears" = paste0(100 * round(p_swears, 4), "%")
  ) |>
  knitr::kable(format = "pipe")

summary_flair |>
  filter(n_unique_users >= 50) |>
  slice_max(order_by = -p_swears,
            n = 10) |>
  mutate(rank = paste0("#", row_number())) |>
  transmute("Rank" = rank,
            "Flair" = counted_flair,
            "Total Comments" = format(n_total_comments, big.mark = ","),
            "% Comments w/ Swears" = paste0(100 * round(p_swears, 4), "%")
  ) |>
  knitr::kable(format = "pipe")
```

Classic BYU.

## Most ref complaints per comment

```{r table3, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
summary_flair |>
  filter(n_unique_users >= 50) |>
  slice_max(order_by = p_ref_comps,
            n = 10) |>
  mutate(rank = paste0("#", row_number())) |>
  transmute("Rank" = rank,
            "Flair" = counted_flair,
            "Total Comments" = format(n_total_comments, big.mark = ","),
            "% Comments w/ Ref Mentions" = paste0(100 * round(p_ref_comps, 4), "%")
  ) |>
  knitr::kable(format = "pipe")
```

I actually checked before last week, and Oklahoma rocketed up from ~50th place to the top ten in the Cincinnati game alone.

# r/CFB 2023 Leaderboards

And finally, the big reveal: who is in the lead to claim this year's National Champion of posting? Here are the top 25 so far this season:

```{r table4, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
summary_users_formatted |>
  knitr::kable(format = "pipe")
```

Additionally, here's the official Sicko Award Top Ten, as it stands now:

```{r table5, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
summary_users |>
  filter(author != "RivalryBot") |>
  slice_max(order_by = n_unique_threads,
            n = 10) |>
  mutate(rank = paste0("#", row_number())) |>
  select(rank, author, counted_flair, n_unique_threads, n_comments) |>
  knitr::kable(format = "pipe")
```

---

I hope you found this interesting! I'm going to keep this going for the rest of the season, so please let me know if there's anything else you'd like me to track. Thanks for reading!
