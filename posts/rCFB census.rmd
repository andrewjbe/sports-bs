---
title: "r/CFB Census [including PGTs]"
output:
  md_document:
    variant: markdown_github
date: "2023-09-26"
---

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
library(dplyr)
library(tidyr)
library(tibble)
library(lubridate)
library(ggplot2)
library(stringr)
library(here)
library(purrr)
library(fs)

devtools::load_all()

# Load data -------------------------------------------------------------------

list_data <- list.files(here("data", "reddit-comment-data", "cfb", "2023"),
                        pattern = "*.rds", full.names = T, recursive = T)
  # str_replace("-2023.*", "") |>
  # str_replace(".*/", "")

all_data <- lapply(list_data, readr::read_rds) |>
  dplyr::bind_rows()

gt_data <- all_data |>
  filter(str_detect(title, "@|vs|v.s") & str_detect(title, "\\[Game Thread"))

pgt_data <- all_data |>
  filter(str_detect(title, "\\[Postgame Thread"))

# Coverage checks --------------------------------------------------------------

# Repeats
test_repeat_data <- list.files(here("data", "reddit-comment-data", "cfb", "2023"),
                        pattern = "*.rds", full.names = T, recursive = T) |>
  str_replace("-2023.*", "") |>
  str_replace(".*/", "")

n_repeats <- test_repeat_data[duplicated(test_repeat_data)]

if(length(n_repeats) != 0){
  for(i in seq(from = 1, to = 10)){
    cli::cli_alert_danger("There's some repeat data in there! This is why you should write to a database!")
  }
}

# Clean data -------------------------------------------------------------------

make_flair <- function(flair_text) {
  # alt_flair_list <- "Paper Bag|Team Chaos|"
  case_when(
    flair_text == "Unflaired" ~ paste0(" ðŸ¤¡"),
    flair_text == "Paper Bag" ~ "[Paper Bag](#l/paperbag)",
    flair_text == "Team Chaos" ~ "[Team Chaos](#l/chaos)",
    # str_detect(flair_text, alt_flair_list) ~ paste0("[", flair_text, "]",
    #               "(#l/", tolower(str_remove_all(flair_text, "\\s|\\&")), ")"),
    TRUE ~ paste0("[", flair_text, "]", 
                  "(#f/", tolower(str_remove_all(flair_text, "\\s|\\&")), ")")
  )
}

all_data_clean <- all_data |>
  sportsBs::clean_rcfb_comments()

gt_data_clean <- all_data_clean |>
  filter(str_detect(title, "@|vs|v.s") & str_detect(title, "\\[Game Thread"))

# pgt_data_clean <- all_data_clean |>
#   filter(str_detect(title, "\\[Postgame Thread"))

# Total threads:
n_gts <- gt_data_clean |> count(title, sort = T) |> nrow() |> format(big.mark = ",")
# n_pgts <- pgt_data_clean |> count(title, sort = T) |> nrow() |> format(big.mark = ",")
n_all <- all_data_clean |> count(title, sort = T) |> nrow() |> format(big.mark = ",")

```

# What is this?

As a side project, I decided to scrape every comment from every game thread this season. I decided to use the data to do a "r/CFB census" to see what I could learn about the different fanbases, and I posted the first update [here](https://old.reddit.com/r/CFB/comments/16srlyr/cfb_flair_census_update_scraping_every_game/) earlier in the season. Now that we're getting into the latter half, I thought I'd post another update.

# The data

So far, I've scraped `r n_gts` game threads with `r nrow(gt_data_clean) |> format(big.mark = ",")` total comments. I actually also have the post game threads as well, which expands the total number of comments to `r nrow(all_data_clean) |> format(big.mark = ",")`, but for now I'm just going to focus on the game threads. If you want to see what this post would look like if I included the PGTs too, you can see that [here]().

Here's a fun graph that shows how many comments people left per hour throughout the whole season: [GRAPH: comments per hour]()

Here's a table with the top ten game threads so far this year by total comments. Colorado seems to be popular for some reason, not sure why.

```{r table6, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
gt_data_clean |>
  count(title_clean, sort = T) |>
  slice_max(order_by = n,
            n = 10) |>
  mutate(n = format(n, big.mark = ","),
         rank = paste0("#", row_number())) |>
  select("Rank" = rank,
         "Thread" = title_clean,
         "Total Comments" = n) |>
  knitr::kable(format = "pipe")
```

This time, I also added up the total number of comments in all of each team's game threads this year in order to find the most talked-about team on r/CFB this year. The top team will absolutely shock you!

```{r table7, echo=FALSE, message=FALSE, warning=FALSE} 
gt_data_clean_longer <- gt_data_clean |>
  rowwise() |>
  mutate(both_teams = list(c(home, away))) |>
  pivot_longer(cols = c(home, away), names_to = c("matchup"), values_to = "team") |>
  rowwise() |>
  mutate(
    opp = if_else(both_teams[1] == team, both_teams[2], both_teams[1])
  )

pop <- gt_data_clean_longer |>
  group_by(team) |>
  summarize(
    n_comments = n(),
    n_threads = n_distinct(title),
    comments_per_thread = n_comments / n_threads
  )

pop |>
  slice_max(order_by = comments_per_thread, n = 10) |>
  mutate(n_comments = format(n_comments, big.mark = ","),
         comments_per_thread = format(round(comments_per_thread, 2), big.mark = ","),
         rank = paste0("#", row_number())) |>
  select("Rank" = rank,
         "Team" = team,
         "Avg. Comments per Game Thread" = comments_per_thread,
         "Total Comments" = n_comments,
         "Total Threads" = n_threads) |>
  knitr::kable(format = "pipe")

```

Colorado has had exactly one (1) game thread so far with less than 15k comments (@ ASU). Here is their full slate thus far:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
gt_data_clean |> 
  filter(home == "Colorado" | away == "Colorado") |> 
  count("Thread" = title_clean) |> 
  arrange(desc(n)) |> 
  mutate(n = format(n, big.mark = ",")) |> 
  rename("Total Comments" = n) |>
  knitr::kable(format = "pipe")
```

```{r calc, echo=FALSE, warning=FALSE, message=FALSE}
# Census =======================================================================
# unique_users <- gt_data_clean |>
unique_users <- all_data_clean |>
  group_by(author) |>
  summarize(
    n_comments = n(),
    n_comments_formatted = format(n_comments, big.mark = ","),
    avg_score = mean(score, na.rm = T),
    n_downvoted = sum(score < 1, na.rm = T),
    p_downvoted = n_downvoted / n_comments,
    n_unique_threads = n_distinct(title_clean, na.rm = T),
    all_primary_flairs = paste0(unique(flair_one), collapse = ", "),
    all_secondary_flairs = paste0(unique(flair_two), collapse = ", "),
    flair_list = list(unique(flair_one)),
    # TODO: replace this with something where instead of first() it's explicitly
    # looping through and pulling out the first one that's not "Unflaired"
    counted_flair = purrr::map_chr(flair_list, first),
    flaired_up = if_else(str_detect(all_primary_flairs, ","), TRUE, FALSE),
    n_swears = sum(swear, na.rm = T),
    p_swears = sum(n_swears) / n_comments,
    n_ref_comps = sum(ref_complaint, na.rm = T),
    p_ref_comps = sum(n_ref_comps) / n_comments,
    n_ads = sum(ad_complaint, na.rm = T),
    p_ads = sum(n_ads) / n_comments
  )

# Top users
summary_users <- unique_users |>
  # slice_max(order_by = n_comments, n = 25)
  arrange(desc(n_comments))

summary_users_formatted  <- summary_users |>
  mutate(
    rank = paste0("#", row_number()),
    p_swears = paste0(100 * round(p_swears, 4), "%"),
    p_ref_comps = paste0(100 * round(p_ref_comps, 4), "%"),
    # p_ads = paste0(100 * round(p_ads, 4), "%")
  ) |>
  slice_max(order_by = n_comments, n = 25) |>
  select(
    "Rank" = rank,
    "Poster" = author,
    "Primary Flair" = counted_flair,
    "Total Comments" = n_comments_formatted,
    # "Avg. Score" = avg_score,
    "Unique Threads" = n_unique_threads,
    "% Comments w/ Swears" = p_swears,
    "% Comments w/ Ref Complaints" = p_ref_comps,
    # "% Comments w/ Ad Complaints" = p_ads
  )

# Flair breakdown --------------------------------------------------------------
summary_flair <- unique_users |>
  group_by(counted_flair) |>
  summarize(
    n_unique_users = n(),
    n_total_comments = sum(n_comments, na.rm = T),
    avg_comments_per_user = n_total_comments / n_unique_users,
    avg_avg_score = mean(avg_score, na.rm = T),
    p_swears = sum(n_swears) / n_total_comments,
    p_ref_comps = sum(n_ref_comps) / n_total_comments,
    # p_ads = sum(n_ads) / n_total_comments
  ) |>
  ungroup() |>
  mutate(
    flair_pic = make_flair(counted_flair)
  )

top_users <- unique_users |>
  group_by(counted_flair) |>
  filter(
    n_comments == max(n_comments)
  ) |>
  mutate(author = paste0(author, " (", n_comments_formatted, " comments)")) |>
  select(counted_flair, "top_poster" = author)

summary_flair_formatted <- summary_flair |>
  filter(n_unique_users >= 50) |>
  arrange(desc(n_unique_users)) |>
  mutate(
    across(.cols = !c(flair_pic, counted_flair, p_swears, p_ref_comps),
           .fns = ~format(round(.x, 2), big.mark = ",")),
    p_swears = paste0(100 * round(p_swears, 4), "%"),
    p_ref_comps = paste0(100 * round(p_ref_comps, 4), "%"),
  ) |>
  # Add top user
  left_join(top_users, by = "counted_flair") |>
  filter(counted_flair != "Unflaired") |>
  mutate(
    rank = paste0("#", row_number())
    ) |>
  # slice_max(n = 100, order_by = n_unique_users) |>
  select(
    "Rank" = rank,
    "Logo" = flair_pic,
    "Primary Flair" = counted_flair,
    "Unique Users" = n_unique_users,
    "Total Comments" = n_total_comments,
    "Comments per User" = avg_comments_per_user,
    "Avg. Comment Score" = avg_avg_score,
    "% of Comments w/ Swears" = p_swears,
    "% of Comments w/ Ref Complaints" = p_ref_comps,
    "Top Poster" = top_poster
  )

# How many flaired up in the dataset?
n_flaired_up <- sum(unique_users$flaired_up)

summary_flaired_up <- unique_users |>
  filter(flaired_up) |>
  count(counted_flair, sort = T) |>
  left_join(summary_flair |>
              select(counted_flair, n_unique_users)) |>
  mutate(new_flairs_percap = n / n_unique_users)

```

# r/CFB flair census

The first question I wanted to answer was "how big is each fanbase?" To determine this, I took every username that left a comment in a game thread this year, and classified them by their most recent primary flair. Also, just to save space, I cut it off at flairs with 50+ unique users. Feel free to ask if you're curious about a smaller school's numbers.

The first table here shows the results, sorted by total unique users:

## Flair census

```{r table1, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
summary_flair_formatted |>
  knitr::kable(format = "pipe")
```

## Top 10 most / least chatty flair

This is measured by the number of comments per unique user made by each flair.

```{r table1_5, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
summary_flair |>
  filter(n_unique_users >= 50) |>
  slice_max(order_by = avg_comments_per_user,
            n = 10) |>
  mutate(rank = paste0("#", row_number())) |>
  transmute("Rank" = rank,
            "Logo" = flair_pic,
            "Flair" = counted_flair,
            "Total Comments" = format(n_total_comments, big.mark = ","),
            # "Total Users" = format(n_unique_users, big.mark = ","),
            "Comments per User" = format(round(avg_comments_per_user, 1), big.mark = ",")
  ) |>
  knitr::kable(format = "pipe")

summary_flair |>
  filter(n_unique_users >= 50) |>
  slice_max(order_by = -avg_comments_per_user,
            n = 10) |>
  mutate(rank = paste0("#", row_number())) |>
  transmute("Rank" = rank,
            "Logo" = flair_pic,
            "Flair" = counted_flair,
            "Total Comments" = format(n_total_comments, big.mark = ","),
            "Comments per User" = format(round(avg_comments_per_user, 1), big.mark = ",")
  ) |>
  knitr::kable(format = "pipe")
```

## Top 10 most / least swears

I scanned each comment for swear words, including all variations of "fuck", "wtf", "ass", "damn", "shit", "hell", "bitch", and "bastard". I also limited this to include only flairs with at least 2,500 total comments in the dataset. Here are the most and least foul-mouthed fanbases:

```{r table2, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
summary_flair |>
  filter(
    n_total_comments >= 2500,
    n_unique_users >= 50
    ) |>
  slice_max(order_by = p_swears,
            n = 10) |>
  mutate(rank = paste0("#", row_number())) |>
  transmute("Rank" = rank,
            "Logo" = flair_pic,
            "Flair" = counted_flair,
            "Total Comments" = format(n_total_comments, big.mark = ","),
            "% Comments w/ Swears" = paste0(100 * round(p_swears, 4), "%")
  ) |>
  knitr::kable(format = "pipe")

summary_flair |>
  filter(
    n_total_comments >= 2500,
    n_unique_users >= 50
    ) |>
  slice_max(order_by = -p_swears,
            n = 10) |>
  mutate(rank = paste0("#", row_number())) |>
  transmute("Rank" = rank,
            "Logo" = flair_pic,
            "Flair" = counted_flair,
            "Total Comments" = format(n_total_comments, big.mark = ","),
            "% Comments w/ Swears" = paste0(100 * round(p_swears, 4), "%")
  ) |>
  knitr::kable(format = "pipe")
```

Classic BYU.

## Most ref complaints per comment

I used a similar approach for this, scanning each comment for words indicating a ref complaint. This isn't perfect, because it misses vaguely worded things like "oh, come on" and can include some false positives like "the refs are doing a great job and I love them", but it's close enough to do the job. Includes variations of terms like "refs" (including "referees", "refball", etc.), "officials", "flag", "whistle", "the fix", "rig", etc. I also limited this to include only flairs with at least 2,500 total comments in the dataset.

```{r table3, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
summary_flair |>
  filter(
    n_total_comments >= 2500,
    n_unique_users >= 50
    ) |>
  slice_max(order_by = p_ref_comps,
            n = 10) |>
  mutate(rank = paste0("#", row_number())) |>
  transmute("Rank" = rank,
            "Logo" = flair_pic,
            "Flair" = counted_flair,
            "Total Comments" = format(n_total_comments, big.mark = ","),
            "% Comments w/ Ref Mentions" = paste0(100 * round(p_ref_comps, 4), "%")
  ) |>
  knitr::kable(format = "pipe")
```

# r/CFB 2023 Leaderboards

And finally, the big reveal: who is in the lead to claim this year's National Champion of posting? Here are the top 25 so far this season:

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
summary_users_formatted |>
  knitr::kable(format = "pipe")
```

BikiniATroll maintains a comfortable lead, as the only poster to break 2k, and is on pace to close out the victory unless someone steps up. Special shoutout to leakymemo for the impressive swear rate.

Additionally, here's the official Sicko Award Top Ten, as it stands now:

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
summary_users |>
  filter(author != "RivalryBot") |>
  slice_max(order_by = n_unique_threads,
            n = 10) |>
  mutate(rank = paste0("#", row_number()),
         n_comments = format(n_comments, big.mark = ",")) |>
  select("Rank" = rank, 
         "Author" = author, 
         "Flair" = counted_flair, 
         "N Threads" = n_unique_threads, 
         "N Comments" = n_comments) |>
  knitr::kable(format = "pipe")
```

This is still anyone's game! 

---

I hope you found this interesting! I'm going to keep this going for the rest of the season, so please let me know if there's anything else you'd like me to track. Thanks for reading!
